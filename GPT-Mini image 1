import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import numpy as np

# ==== VAE para GPT-Mini Image 1 ====
class GPTMiniImageVAE(nn.Module):
    def __init__(self, image_size=3840, num_tokens=8192, dim=512, depth=3, heads=8, dim_head=64):
        super().__init__()
        self.image_size = image_size
        self.num_tokens = num_tokens
        self.dim = dim
        self.depth = depth
        self.heads = heads
        self.dim_head = dim_head

        # Encoder
        self.encoder = nn.Sequential(
            nn.Conv2d(3, dim, 4, 2, 1),
            nn.ReLU(),
            nn.Conv2d(dim, dim * 2, 4, 2, 1),
            nn.ReLU(),
            nn.Conv2d(dim * 2, dim * 4, 4, 2, 1),
            nn.ReLU(),
            nn.Conv2d(dim * 4, dim * 8, 4, 2, 1),
            nn.ReLU(),
            nn.Conv2d(dim * 8, dim * 16, 4, 2, 1),
            nn.ReLU(),
            nn.Conv2d(dim * 16, dim * 32, 4, 2, 1)
        )

        self.to_latent = nn.Conv2d(dim * 32, dim, 1)

        # Decoder
        self.to_image = nn.Sequential(
            nn.ConvTranspose2d(dim, dim * 16, 4, 2, 1),
            nn.ReLU(),
            nn.ConvTranspose2d(dim * 16, dim * 8, 4, 2, 1),
            nn.ReLU(),
            nn.ConvTranspose2d(dim * 8, dim * 4, 4, 2, 1),
            nn.ReLU(),
            nn.ConvTranspose2d(dim * 4, dim * 2, 4, 2, 1),
            nn.ReLU(),
            nn.ConvTranspose2d(dim * 2, dim, 4, 2, 1),
            nn.ReLU(),
            nn.ConvTranspose2d(dim, 3, 4, 2, 1)
        )

    def forward(self, x):
        x = self.encoder(x)
        x = self.to_latent(x)
        x = self.to_image(x)
        return x

# ==== GPT-Mini Image 1 ====
class GPTMiniImage1(nn.Module):
    def __init__(self, vae, num_tokens=8192, dim=512, depth=3, heads=8, dim_head=64):
        super().__init__()
        self.vae = vae
        self.num_tokens = num_tokens
        self.dim = dim
        self.depth = depth
        self.heads = heads
        self.dim_head = dim_head

        # Embeddings e Transformer
        self.token_embedding = nn.Embedding(num_tokens, dim)
        self.pos_embedding = nn.Parameter(torch.randn(1, 256, dim))
        self.transformer = nn.Transformer(dim, heads, depth)
        self.to_image = nn.Linear(dim, 3 * vae.image_size * vae.image_size)

    def forward(self, text, image):
        text_emb = self.token_embedding(text)
        pos_emb = self.pos_embedding[:, :text.size(1)]
        x = text_emb + pos_emb
        x = self.transformer(x)
        x = x.mean(dim=1)
        x = self.to_image(x)
        x = x.view(-1, 3, self.vae.image_size, self.vae.image_size)
        return x

# ==== Dataset fictício ====
class GPTMiniImageDataset(Dataset):
    def __init__(self, num_samples=1000, image_size=3840):
        self.num_samples = num_samples
        self.image_size = image_size

    def __len__(self):
        return self.num_samples

    def __getitem__(self, idx):
        text = torch.randint(0, 8192, (256,))
        image = torch.randn(3, self.image_size, self.image_size)
        return text, image

# ==== Treinamento ====
def train_gpt_mini_image1():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    vae = GPTMiniImageVAE().to(device)
    model = GPTMiniImage1(vae).to(device)
    dataset = GPTMiniImageDataset()
    dataloader = DataLoader(dataset, batch_size=2, shuffle=True)
    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
    criterion = nn.MSELoss()

    for epoch in range(10):
        for text, image in dataloader:
            text, image = text.to(device), image.to(device)
            optimizer.zero_grad()
            output = model(text, image)
            loss = criterion(output, image)
            optimizer.step()
        print(f"Epoch {epoch + 1} - Loss: {loss.item():.4f}")

    print("✅ Treinamento do GPT-Mini Image 1 concluído!")

if __name__ == "__main__":
    train_gpt_mini_image1()
